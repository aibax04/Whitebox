from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
import os
import subprocess
import re
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai
import asyncio

app = FastAPI()

# Add CORS support
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

STATUS_PATH = "/Users/psiadmin/clawd/workspace/whitebox-dashboard/frontend/public/status.json"
COMMANDS_LOG = "/Users/psiadmin/clawd/workspace/whitebox-dashboard/commands.log"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, "../../simpliautomate_new/.env")
load_dotenv(ENV_PATH)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

MODEL_NAME = "gemini-1.5-flash" 

from bridge import bridge

class ActionRequest(BaseModel):
    review_id: str
    action: str # approve, reject

class SkillRequest(BaseModel):
    topic: str

class ChatRequest(BaseModel):
    message: str

async def process_chat_logic(msg):
    # 1. Detect Repository URL
    repo_match = re.search(r'https://github\.com/[\w.-]+/([\w.-]+)', msg)
    
    if repo_match:
        repo_url = repo_match.group(0)
        repo_name = repo_match.group(1)
        
        # Load status safely
        try:
            with open(STATUS_PATH, 'r') as f:
                data = json.load(f)
        except:
            data = {"workflow": {"history": []}}

        # Mission Log
        log_entries = [
            f"üöÄ [SQUAD MISSION]: Expansion Protocol Initiated for {repo_name}",
            f"‚è≥ [STAGE 1/4]: Cloning repository {repo_url}...",
            f"üõ°Ô∏è [SECURITY]: Scanning codebase for vulnerabilities..."
        ]
        
        for entry in log_entries:
            data["workflow"]["history"].insert(0, entry)
            
        ai_response = f"Repository **{repo_name}** detected. ‚¨ú\n\nI have initiated the **SquadRun Expansion Protocol**."

        if GEMINI_API_KEY:
            try:
                model = genai.GenerativeModel(MODEL_NAME)
                prompt = f"""
                You are Squad Mate, an advanced autonomous coding agent. 
                The user has requested to deploy the GitHub repository: {repo_url}.
                
                Please generate a brief, tactical mission plan that outlines:
                1. Cloning and analyzing the codebase.
                2. Resolving dependencies (npm/pip).
                3. Launching the environment.
                
                Formatted as a markdown response for the dashboard chat.
                Keep it professional, high-tech, and concise.
                """
                response = await asyncio.to_thread(model.generate_content, prompt)
                if response.text:
                    ai_response += "\n\n" + response.text
                    data["workflow"]["history"].insert(0, f"üß† [CORTEX]: Mission plan generated by {MODEL_NAME}.")
            except Exception as ai_e:
                print(f"Gemini generation failed: {ai_e}")
                ai_response += "\n\n(Neural Link unstable - reverting to default protocol)"
        
        # Save status safely
        tmp_path = STATUS_PATH + ".tmp"
        with open(tmp_path, 'w') as f:
            json.dump(data, f, indent=4)
        os.rename(tmp_path, STATUS_PATH)

        try:
            subprocess.Popen([
                "openclaw", "sessions", "send", "agent:main:main", 
                f"Execute SquadRun Expansion Protocol for {repo_url}. Report progress."
            ])
        except:
            pass

        return {
            "role": "assistant",
            "content": ai_response
        }
    
    return {
        "role": "assistant",
        "content": f"Mission parameters received. I've logged '{msg}' for processing."
    }

@app.post("/chat")
@app.post("/command")
async def handle_mission_command(request: ChatRequest):
    """
    Unified entry point for natural language commands and repository expansion.
    Actually connects to Acknowledge, Simpliautomate, and PredCo.
    """
    try:
        msg = request.message
        msg_lower = msg.lower()
        
        # 1. Acknowledge Integration logic
        if "notify" in msg_lower or "announce" in msg_lower:
            content = msg.split("notify", 1)[-1].split("announce", 1)[-1].strip()
            res = bridge.send_ack_notification("Whitebox Manual Override", content)
            if res.get("error"):
                bridge.trigger_nonstop_sync()
                return {"role": "assistant", "content": "‚ö†Ô∏è **Primary Bridge Failure.** Nonstop Agent has intercepted the request and is preparing a local dummy environment."}
            return {"role": "assistant", "content": f"‚úÖ **Acknowledge Bridge Active.** Broadcast dispatched: \"{content}\""}
        
        if "task" in msg_lower and ("create" in msg_lower or "assign" in msg_lower):
            title = msg.replace("create task", "").replace("assign task", "").strip()
            res = bridge.create_ack_task(title, "Mission assigned via Whitebox Command Center.")
            if res.get("error"):
                bridge.trigger_nonstop_sync()
                return {"role": "assistant", "content": "‚ö†Ô∏è **Mission Registration Failed.** Nonstop Agent has registered the task locally to ensure continuity."}
            return {"role": "assistant", "content": f"‚úÖ **Mission Registered.** Task created in Acknowledge: **{title}**"}

        # 2. Simpliautomate Integration logic
        if "news" in msg_lower or "simplii" in msg_lower:
            news = bridge.fetch_simplii_news()
            if isinstance(news, dict) and news.get("error"):
                bridge.trigger_nonstop_sync()
                return {"role": "assistant", "content": "üìä **Social Core Unstable.** Nonstop Agent is pulling archived trend data for simulation."}
            count = len(news) if isinstance(news, list) else 0
            return {"role": "assistant", "content": f"üìä **Simplii Sync Active.** Found **{count}** live trending news items from the social core."}

        # 3. PredCo Integration logic
        if "compliance" in msg_lower or "predco" in msg_lower:
            stats = bridge.get_predco_dashboard()
            if stats.get("error"):
                bridge.trigger_nonstop_sync()
                return {"role": "assistant", "content": "üõ°Ô∏è **Compliance Bridge Unstable.** Nonstop Agent has secured a local snapshot of regulatory data."}
            doc_count = stats.get("stats", {}).get("documents_processed", 0)
            return {"role": "assistant", "content": f"üõ°Ô∏è **Compliance Bridge Secured.** System is currently monitoring **{doc_count}** active regulatory documents."}

        # 4. Nonstop Data Maintenance logic
        if "nonstop" in msg_lower or "sync data" in msg_lower or "dummy data" in msg_lower:
            res = bridge.trigger_nonstop_sync()
            return {"role": "assistant", "content": "üîÑ **Nonstop Agent Activated.** Re-organizing local data stores and seeding dummy data for ecosystem stability."}

        # 5. Fallback to Repository expansion logic
        return await process_chat_logic(msg)
        
    except Exception as e:
        print(f"Mission Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/learn")
async def handle_learning(request: SkillRequest):
    try:
        subprocess.Popen([
            "python3", "/Users/psiadmin/clawd/workspace/whitebox-dashboard/backend/autonomous_pilot.py", request.topic
        ])
        return {"status": "success", "message": f"Autonomous learning mission started for: {request.topic}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/action")
async def handle_action(request: ActionRequest):
    try:
        with open(STATUS_PATH, 'r') as f:
            data = json.load(f)
        found_review = None
        if "reviews" in data:
            for r in data["reviews"]:
                if r["id"] == request.review_id:
                    found_review = r
                    break
            if found_review:
                data["reviews"] = [r for r in data["reviews"] if r["id"] != request.review_id]
                log_entry = f"User {request.action.upper()}ED: {found_review['title']}"
                data["workflow"]["history"].insert(0, log_entry)
                with open(COMMANDS_LOG, 'a') as f:
                    f.write(f"{datetime.now().isoformat()} | {request.review_id} | {request.action} | {found_review['title']}\n")
                msg = f"‚úÖ [DASHBOARD ACTION] User has {request.action.upper()}ED the deployment: {found_review['title']}"
                subprocess.run(["openclaw", "message", "send", "--to", "1707270118", "--message", msg])
        with open(STATUS_PATH, 'w') as f:
            json.dump(data, f, indent=4)
        return {"status": "success", "message": f"Action {request.action} processed for {request.review_id}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "ok", "message": "Universal Neural Bridge is operational"}

async def nonstop_loop():
    """Background loop for the Nonstop agent to keep data fresh."""
    while True:
        try:
            # Trigger the nonstop data engine
            bridge.trigger_nonstop_sync()
        except Exception as e:
            print(f"Nonstop loop error: {e}")
        await asyncio.sleep(1800) # Keep adding data every 30 minutes

@app.on_event("startup")
async def startup_event():
    # Start the Nonstop background loop
    asyncio.create_task(nonstop_loop())

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=35002)
